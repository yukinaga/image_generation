{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6tVYkCYQfODlJ9CDEEGWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/image_generation/blob/main/section_3/01_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K74mqS_L8nq4"
      },
      "source": [
        "# オートエンコーダの実装\n",
        "VAEの実装に入る前に、通常のオートエンコーダを実装しましょう。   \n",
        "Encoderで中間層に画像を圧縮した後に、Decoderで元の画像を再構築します。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7h50pYSRaZ0"
      },
      "source": [
        "## 手書き文字画像\n",
        "今回は、オートエンコーダにより手書き文字画像を圧縮、復元します。  \n",
        "scikit-learnから、8×8の手書き数字の画像データを読み込んで表示します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc9WCis8TGwx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "digits_data = datasets.load_digits()\n",
        "\n",
        "n_img = 10  # 表示する画像の数\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n_img):\n",
        "    # 入力画像\n",
        "    ax = plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(digits_data.data[i].reshape(8, 8), cmap=\"Greys_r\")\n",
        "    ax.get_xaxis().set_visible(False)  # 軸を非表示に\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "print(\"データの形状:\", digits_data.data.shape)\n",
        "print(\"ラベル:\", digits_data.target[:n_img])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtujqB1gSh4I"
      },
      "source": [
        "## 各設定\n",
        "画像の幅と高さが8ピクセルなので、入力層には8×8=64のニューロンが必要になります。  \n",
        "また、出力が入力を再現するように学習するので、出力層のニューロン数は入力層と同じになります。  \n",
        "中間層にはこれらよりも少ないニューロンを配置します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbSwlXZIUN7C"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -- 各設定値 --\n",
        "img_size = 8  # 画像の高さと幅\n",
        "n_in_out = img_size * img_size  # 入出力層のニューロン数\n",
        "n_mid = 16  # 中間層のニューロン数\n",
        "\n",
        "eta = 0.01  # 学習係数\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "interval = 10  # 経過の表示間隔\n",
        "\n",
        "# -- 訓練データ --\n",
        "digits_data = datasets.load_digits()\n",
        "x_train = np.asarray(digits_data.data)\n",
        "x_train /= 16  # 0-1の範囲に\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float)\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYb9yszSURKH"
      },
      "source": [
        "## モデルの構築\n",
        "PyTorchよりオートエンコーダのモデルを構築します。  \n",
        "Encoder、Decoderの順に層を重ねます。  \n",
        "入力の値は0から1の範囲なのですが、出力の範囲をこれに合わせる必要があります。  \n",
        "従って、出力層の活性化関数には出力範囲が0から1に収まるシグモイド関数を使います。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qfDQ8QJolyo"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(n_in_out, n_mid)  # Encoder\n",
        "        self.decoder = nn.Linear(n_mid, n_in_out)  # Decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, n_in_out)  # バッチサイズ×入力の数\n",
        "        x = F.relu(self.encoder(x))\n",
        "        x = F.sigmoid(self.decoder(x))\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = x.view(-1, n_in_out)  # バッチサイズ×入力の数\n",
        "        x = F.relu(self.encoder(x))\n",
        "        return x\n",
        "\n",
        "autoencoder = Autoencoder()\n",
        "autoencoder.cuda()  # GPU対応\n",
        "print(autoencoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC4f8NH6U86F"
      },
      "source": [
        "## 学習\n",
        "構築したオートエンコーダのモデルを使って、学習を行います。  \n",
        "入力を再現するように学習するので、正解は入力そのものになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoOAs3rh2PJJ"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# 二乗和誤差\n",
        "loss_fnc = nn.MSELoss()\n",
        "\n",
        "# Adam\n",
        "optimizer = optim.Adam(autoencoder.parameters())\n",
        "\n",
        "# 損失のログ\n",
        "record_loss_train = []\n",
        "\n",
        "# 学習\n",
        "for i in range(epochs):\n",
        "    autoencoder.train()  # 訓練モード\n",
        "    loss_train = 0\n",
        "    for j, (x,) in enumerate(train_loader):  # ミニバッチ（x,）を取り出す\n",
        "        x = x.cuda()  # GPU対応\n",
        "        y = autoencoder(x)\n",
        "        loss = loss_fnc(y, x)\n",
        "        loss_train += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_train /= j+1\n",
        "    record_loss_train.append(loss_train)\n",
        "\n",
        "    if i%interval == 0:\n",
        "        print(\"Epoch:\", i, \"Loss_Train:\", loss_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6dNqM86bh3U"
      },
      "source": [
        "## 誤差の推移\n",
        "記録された誤差の、推移を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzuFBDy37tRg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn-zFGCFWb3I"
      },
      "source": [
        "誤差が滑らかに減少している様子が確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dypLOxuaWhoE"
      },
      "source": [
        "## 生成された画像の表示\n",
        "画像が適切に再構築されているかどうか、中間層がどのような状態にあるのかを確認します。  \n",
        "入力画像と、再構築された画像を並べて表示します。  \n",
        "また、エンコーダーの出力も4×4の画像として表示します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVhJFHgTBffI"
      },
      "source": [
        "n_img = batch_size  # 表示する画像の数\n",
        "x = next(iter(train_loader))[0]\n",
        "x = x.cuda()\n",
        "\n",
        "autoencoder.eval()  # 評価モード\n",
        "m = autoencoder.encode(x)  # 中間層の状態\n",
        "y = autoencoder(x)\n",
        "\n",
        "# NumPyの配列に変換\n",
        "x_sample = x.cpu().detach().numpy()\n",
        "m_sample = m.cpu().detach().numpy()\n",
        "y_sample = y.cpu().detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(n_img, 3))\n",
        "for i in range(n_img):\n",
        "    # 入力画像\n",
        "    ax = plt.subplot(3, n_img, i+1)\n",
        "    plt.imshow(x_sample[i].reshape(img_size, -1).tolist(), cmap=\"Greys_r\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # 中間層の出力\n",
        "    ax = plt.subplot(3, n_img, i+1+n_img)\n",
        "    plt.imshow(m_sample[i].reshape(4, -1).tolist(), cmap=\"Greys_r\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # 出力画像\n",
        "    ax = plt.subplot(3, n_img, i+1+2*n_img)\n",
        "    plt.imshow(y_sample[i].reshape(img_size, -1).tolist(), cmap=\"Greys_r\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qBV-ARLgwZi"
      },
      "source": [
        "出力は入力をある程度再現した画像になっており、中間層は数字画像ごとに異なる状態となっています。  \n",
        "64ピクセルの画像を特徴付ける情報を、16の状態に圧縮できたことになります。  \n",
        "しかしながら、中間層の状態と出力画像の対応関係を直感的に把握したり、中間層の状態を調整して出力画像を変化させることは難しそうです。  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr0qZ81oX3Bk"
      },
      "source": [
        "## 演習\n",
        "中間層のニューロン数が少なくなると、次第にうまく画像を再構築できなくなります。  \n",
        "実際にニューロン数を少なくし、再構築ができなくなることを確認しましょう。"
      ]
    }
  ]
}